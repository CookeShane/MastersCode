{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Edge Generation using DexiNED"]},{"cell_type":"markdown","metadata":{},"source":["***\n","\n","**Author:** Shane Cooke\n","\n","**Date:** 3 Dec 2022\n","\n","**Description:** DexiNED finds the edges of the images in 'data' folder and saves the edge maps in 'results' folder\n","***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"markdown","metadata":{},"source":["#### Install Dependencies"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5870,"status":"ok","timestamp":1659542136409,"user":{"displayName":"Shane Cooke","userId":"07078172024541208594"},"user_tz":-60},"id":"R8HDoY5qip-N","outputId":"ea85b40f-b87d-4bb4-b8b2-e9ad1dcf8f3f"},"outputs":[],"source":["#!pip install kornia"]},{"cell_type":"markdown","metadata":{},"source":["#### Imports"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2144,"status":"ok","timestamp":1659542138527,"user":{"displayName":"Shane Cooke","userId":"07078172024541208594"},"user_tz":-60},"id":"T6Hu0d5oizKM"},"outputs":[],"source":["import kornia\n","import os\n","import cv2\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["***\n","## Edge Generation"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["!rm -rf ./DexiNed/data\n","!mkdir ./DexiNed/data\n","\n","!rm -rf ./DexiNed/result\n","!mkdir ./DexiNed/result\n","\n","!rm -rf ./Dataset/ASL_EDGE_MAP/Val\n","!mkdir ./Dataset/ASL_EDGE_MAP/Val"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["!cp -a ./Dataset/Augments/CROP/Val/. ./DexiNed/data/"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/shane/Documents/MastersCode/DexiNed\n"]}],"source":["%cd DexiNed"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20474,"status":"ok","timestamp":1659542270006,"user":{"displayName":"Shane Cooke","userId":"07078172024541208594"},"user_tz":-60},"id":"-T3k4ozpXG5Q","outputId":"628678ab-9b14-4f50-925c-eea10ce20b99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of GPU's available: 1\n","Pytorch version: 1.12.1\n","mean_bgr: [103.939, 116.779, 123.68]\n","/home/shane/anaconda3/envs/masters/lib/python3.10/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","output_dir: result/BIPED2CLASSIC\n","Restoring weights from: checkpoints/BIPED/10/10_model.pth\n","actual size: (262, 176, 3), target size: (512, 512)\n","actual size: (214, 203, 3), target size: (512, 512)\n","actual size: (209, 199, 3), target size: (512, 512)\n","actual size: (201, 235, 3), target size: (512, 512)\n","actual size: (119, 88, 3), target size: (512, 512)\n","actual size: (37, 23, 3), target size: (512, 512)\n","actual size: (182, 162, 3), target size: (512, 512)\n","actual size: (146, 105, 3), target size: (512, 512)\n","actual size: (288, 277, 3), target size: (512, 512)\n","actual size: (35, 24, 3), target size: (512, 512)\n","actual size: (95, 51, 3), target size: (512, 512)\n","actual size: (188, 202, 3), target size: (512, 512)\n","actual size: (251, 178, 3), target size: (512, 512)\n","actual size: (175, 133, 3), target size: (512, 512)\n","actual size: (189, 142, 3), target size: (512, 512)\n","actual size: (212, 173, 3), target size: (512, 512)actual size: (231, 200, 3), target size: (512, 512)\n","actual size: (304, 202, 3), target size: (512, 512)actual size: (247, 277, 3), target size: (512, 512)\n","actual size: (337, 236, 3), target size: (512, 512)actual size: (244, 216, 3), target size: (512, 512)\n","actual size: (179, 162, 3), target size: (512, 512)\n","actual size: (204, 189, 3), target size: (512, 512)\n","\n","\n","\n","actual size: (170, 145, 3), target size: (512, 512)\n","actual size: (157, 116, 3), target size: (512, 512)\n","actual size: (196, 156, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (276, 220, 3), target size: (512, 512)\n","actual size: (264, 197, 3), target size: (512, 512)\n","actual size: (214, 138, 3), target size: (512, 512)\n","actual size: (266, 321, 3), target size: (512, 512)\n","actual size: (208, 221, 3), target size: (512, 512)\n","actual size: (259, 223, 3), target size: (512, 512)\n","actual size: (315, 288, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (252, 209, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (258, 247, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (187, 361, 3), target size: (512, 512)\n","actual size: (184, 133, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (58, 33, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (162, 142, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (197, 189, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (344, 261, 3), target size: (512, 512)\n","\n","actual size: (196, 175, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (202, 220, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (266, 215, 3), target size: (512, 512)\n","actual size: (41, 21, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (231, 163, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (247, 199, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (83, 56, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (241, 222, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (333, 206, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (370, 175, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (199, 212, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (237, 214, 3), target size: (512, 512)\n","actual size: (152, 93, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (174, 126, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (154, 144, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (34, 23, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (224, 259, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (183, 160, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (306, 271, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (206, 163, 3), target size: (512, 512)\n","actual size: (111, 77, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (177, 162, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (257, 291, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (270, 101, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (335, 232, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (319, 235, 3), target size: (512, 512)\n","\n","actual size: (144, 87, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (40, 27, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (229, 165, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (177, 152, 3), target size: (512, 512)input tensor shape: torch.Size([1, 3, 464, 400])\n","\n","actual size: (225, 176, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (155, 155, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (163, 222, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (275, 201, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (251, 131, 3), target size: (512, 512)\n","actual size: (256, 77, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (185, 170, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (190, 166, 3), target size: (512, 512)\n","actual size: (34, 24, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (166, 139, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (232, 167, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (204, 205, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (278, 258, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (447, 242, 3), target size: (512, 512)\n","actual size: (234, 204, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (196, 135, 3), target size: (512, 512)input tensor shape: torch.Size([1, 3, 464, 400])\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (342, 287, 3), target size: (512, 512)\n","\n","actual size: (221, 146, 3), target size: (512, 512)input tensor shape: torch.Size([1, 3, 464, 400])\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (173, 208, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (248, 183, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (271, 220, 3), target size: (512, 512)\n","\n","actual size: (376, 213, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (153, 97, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (165, 154, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (262, 321, 3), target size: (512, 512)\n","\n","actual size: (303, 212, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (278, 150, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (259, 204, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (172, 146, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (367, 265, 3), target size: (512, 512)\n","actual size: (205, 172, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (251, 178, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (261, 173, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (212, 190, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (385, 263, 3), target size: (512, 512)\n","actual size: (278, 213, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (248, 222, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (139, 141, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (207, 183, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (200, 157, 3), target size: (512, 512)input tensor shape: torch.Size([1, 3, 464, 400])\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (297, 236, 3), target size: (512, 512)\n","\n","actual size: (249, 229, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (220, 216, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (138, 132, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (216, 212, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (291, 160, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (296, 165, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (194, 169, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (239, 210, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (171, 186, 3), target size: (512, 512)\n","actual size: (196, 145, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (352, 406, 3), target size: (512, 512)\n","\n","actual size: (266, 169, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (355, 270, 3), target size: (512, 512)\n","\n","actual size: (143, 94, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (249, 219, 3), target size: (512, 512)input tensor shape: torch.Size([1, 3, 464, 400])\n","\n","actual size: (190, 149, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (278, 51, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (155, 140, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (185, 173, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (281, 209, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (313, 187, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (165, 250, 3), target size: (512, 512)\n","actual size: (96, 60, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (175, 128, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (177, 76, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (296, 191, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (301, 187, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (205, 203, 3), target size: (512, 512)\n","actual size: (295, 208, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (238, 172, 3), target size: (512, 512)\n","\n","actual size: (181, 128, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (220, 174, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (385, 289, 3), target size: (512, 512)\n","actual size: (279, 209, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (249, 244, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (169, 177, 3), target size: (512, 512)\n","actual size: (167, 135, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (112, 55, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (256, 189, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (280, 266, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (219, 193, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (199, 186, 3), target size: (512, 512)\n","\n","actual size: (36, 23, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (253, 299, 3), target size: (512, 512)\n","\n","actual size: (195, 137, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (316, 281, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (225, 220, 3), target size: (512, 512)\n","\n","actual size: (233, 210, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (233, 190, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (282, 200, 3), target size: (512, 512)\n","actual size: (201, 144, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (139, 60, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (202, 113, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (236, 165, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (216, 233, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (68, 40, 3), target size: (512, 512)\n","actual size: (222, 318, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (289, 184, 3), target size: (512, 512)\n","\n","actual size: (211, 209, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (259, 344, 3), target size: (512, 512)\n","\n","actual size: (223, 167, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (225, 143, 3), target size: (512, 512)\n","\n","actual size: (224, 213, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (296, 311, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (251, 286, 3), target size: (512, 512)\n","\n","actual size: (212, 204, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (177, 195, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (181, 137, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (157, 108, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (48, 28, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (304, 144, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (268, 252, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (267, 140, 3), target size: (512, 512)\n","actual size: (300, 188, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (350, 225, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (239, 232, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (269, 181, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (144, 140, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (262, 258, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (233, 294, 3), target size: (512, 512)\n","actual size: (192, 156, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (226, 237, 3), target size: (512, 512)\n","\n","actual size: (143, 91, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (296, 266, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (217, 160, 3), target size: (512, 512)\n","actual size: (170, 137, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (228, 273, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (211, 196, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (212, 178, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (83, 58, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (220, 218, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (197, 187, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (256, 179, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (203, 146, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (228, 231, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (158, 142, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (107, 54, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (243, 141, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (325, 228, 3), target size: (512, 512)\n","actual size: (139, 95, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (247, 194, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (214, 172, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (50, 28, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (44, 25, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (288, 275, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (276, 219, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (209, 187, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (267, 140, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (234, 176, 3), target size: (512, 512)\n","\n","actual size: (269, 160, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (233, 168, 3), target size: (512, 512)\n","actual size: (262, 262, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (142, 121, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (265, 283, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (257, 254, 3), target size: (512, 512)\n","actual size: (173, 180, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (48, 34, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (190, 188, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (179, 161, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (233, 322, 3), target size: (512, 512)\n","\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (327, 230, 3), target size: (512, 512)\n","actual size: (189, 160, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (43, 26, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (182, 149, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (212, 214, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (223, 206, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (299, 217, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (149, 238, 3), target size: (512, 512)\n","actual size: (257, 230, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (375, 212, 3), target size: (512, 512)\n","\n","actual size: (174, 235, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (229, 286, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (263, 213, 3), target size: (512, 512)\n","actual size: (131, 89, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (203, 181, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (207, 224, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (197, 220, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (187, 181, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (157, 144, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (234, 190, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (267, 262, 3), target size: (512, 512)\n","actual size: (271, 279, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (198, 167, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (181, 134, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])actual size: (227, 209, 3), target size: (512, 512)\n","\n","actual size: (210, 160, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (378, 315, 3), target size: (512, 512)\n","actual size: (227, 209, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (198, 156, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","actual size: (310, 201, 3), target size: (512, 512)\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","input tensor shape: torch.Size([1, 3, 464, 400])\n","******** Testing finished in CLASSIC dataset. *****\n","FPS: 21.475122.4\n","-------------------------------------------------------\n","Number of parameters of current DexiNed model:\n","35215245\n","-------------------------------------------------------\n"]}],"source":["!python main.py"]},{"cell_type":"markdown","metadata":{},"source":["#### Move Output Edge Maps into Dataset Folder"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/shane/Documents/MastersCode\n"]}],"source":["%cd ..\n","!cp -a ./DexiNed/result/BIPED2CLASSIC/avg/. ./Dataset/ASL_EDGE_MAP/Val"]},{"cell_type":"markdown","metadata":{},"source":["***\n","## Edge Map Augmentation"]},{"cell_type":"markdown","metadata":{},"source":["#### Create Paths"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["dataset = 'Val'\n","\n","input_image_path = f'./Dataset/Augments/CROP/{dataset}/'\n","input_edge_path = f\"./Dataset/ASL_EDGE_MAP/{dataset}/\"\n","\n","output_overlay_path = f\"./Dataset/Augments/EDGE_B/{dataset}/\"\n","os.makedirs(output_overlay_path,exist_ok=True)\n","\n","output_thinned_overlay_path = f\"./Dataset/Augments/EDGE_B/THIN/{dataset}/\"\n","os.makedirs(output_thinned_overlay_path,exist_ok=True)\n","\n","for filename in os.listdir(input_image_path):\n","    if filename.endswith('.JPG'):\n","        os.rename(os.path.join(input_image_path, filename), os.path.join(input_image_path, filename[:-3] + \"jpg\"))\n","\n","jpg_files = [pos_jpg for pos_jpg in os.listdir(input_image_path) if pos_jpg.endswith('.jpg')]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Required Functions"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["def overlay(image, edge, directory, name):\n","    \n","    #edge[np.all(edge == (255,255,255), axis=-1)] = (255,255,0)\n","    edge = cv2.bitwise_not(edge)\n","    edge_array = Image.fromarray(edge)\n","    edge_rgba = edge_array.convert(\"RGBA\")\n","    datas = edge_rgba.getdata()\n","\n","    newData = []\n","    for item in datas:\n","        if item[0] == 255 and item[1] == 255 and item[2] == 255:\n","            newData.append((0, 0, 0, 0))\n","        else:\n","            newData.append(item)\n","\n","    edge_rgba.putdata(newData)\n","    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    image_array = Image.fromarray(image_bgr)\n","    image_array.paste(edge_rgba, (0, 0), edge_rgba)\n","    image_array.save(f'{directory}{name}')"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["def thin_edge(edge):\n","    gray_edge = cv2.cvtColor(edge, cv2.COLOR_BGR2GRAY)\n","    gray_edge_copy = gray_edge.copy()\n","\n","    kernel = cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3))\n","    thinned_edge = np.zeros(gray_edge_copy.shape, dtype='uint8')\n","\n","    while(cv2.countNonZero(gray_edge_copy) != 0):\n","        erode = cv2.erode(gray_edge_copy, kernel)\n","        opening = cv2.morphologyEx(erode, cv2.MORPH_OPEN, kernel)\n","        subset = erode - opening\n","        thinned_edge = cv2.bitwise_or(subset, thinned_edge)\n","        gray_edge_copy = erode.copy()\n","\n","    thinned_edge_bgr = cv2.cvtColor(thinned_edge, cv2.COLOR_GRAY2BGR)\n","\n","    return thinned_edge_bgr"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Convert to binary, overlay, thin & overlay"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["for jpg_image in jpg_files:\n","    \n","    png_image = jpg_image.replace(\".jpg\",\".png\")\n","    jpg_image_read = cv2.imread(f'{input_image_path}{jpg_image}')\n","    png_image_read = cv2.imread(f'{input_edge_path}{png_image}')\n","\n","    inverted_image = cv2.bitwise_not(png_image_read)\n","\n","    ret, binary = cv2.threshold(inverted_image, 100, 255, cv2.THRESH_BINARY)\n","    bw = cv2.threshold(inverted_image, 100, 255, cv2.THRESH_BINARY)\n","\n","    overlay(jpg_image_read, binary, output_overlay_path, jpg_image)\n","\n","    thinned_edge_bgr = thin_edge(binary)\n","\n","    overlay(jpg_image_read, thinned_edge_bgr, output_thinned_overlay_path, jpg_image)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOIw4Cvqqz0r8tpHuxraSOj","collapsed_sections":[],"name":"DexiNED_test.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('masters')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"24d1aeea1e4d3f699033aa98f0aed530c9ef1015c9b5c3418b9321669119b1b4"}}},"nbformat":4,"nbformat_minor":0}
